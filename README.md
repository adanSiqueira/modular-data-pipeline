# Pipeline de Processamento e UnificaÃ§Ã£o de Dados

Este projeto consiste em uma **pipeline automatizada** que realiza o download, carregamento, transformaÃ§Ã£o e unificaÃ§Ã£o de dados provenientes de arquivos `JSON` e `CSV`. Ao final do processo, os dados tratados e padronizados sÃ£o armazenados em um Ãºnico arquivo `CSV`, pronto para anÃ¡lise ou integraÃ§Ã£o em outros sistemas.

## Principais Funcionalidades

âœ… Download automÃ¡tico de arquivos a partir de URLs  
âœ… Estrutura modular e organizada em scripts independentes  
âœ… TransformaÃ§Ã£o de dados: renomeaÃ§Ã£o e padronizaÃ§Ã£o de colunas  
âœ… UnificaÃ§Ã£o de mÃºltiplas fontes de dados em um Ãºnico dataset  
âœ… Armazenamento seguro e organizado dos arquivos brutos e processados  

## Estrutura do Projeto
```
pipeline_dados/
â”œâ”€â”€ data_processed/ # Arquivo final, tratado e unificado
â”œâ”€â”€ data_raw/ # Arquivos brutos, baixados da internet
â”œâ”€â”€ notebooks/ # Notebook exploratÃ³rio inicial
â”œâ”€â”€ scripts/ # Scripts principais da pipeline
â”‚ â”œâ”€â”€ download_dados.py # Realiza o download automatizado dos arquivos
â”‚ â”œâ”€â”€ processamento_dados.py # Classe com mÃ©todos para carregar, transformar e salvar os dados
â”‚ â”œâ”€â”€ fusao_arquivos.py # Script que executa a fusÃ£o dos arquivos
â”‚ â””â”€â”€ executar_pipeline.py # Script que executa toda a pipeline de ponta a ponta
â””â”€â”€ README.md # DocumentaÃ§Ã£o do projeto
```


## ğŸ—ï¸ Como Funciona a Pipeline

1. **Download**: baixa automaticamente os arquivos JSON e CSV a partir de links pÃºblicos, salvando-os na pasta `data_raw`.
2. **Carregamento**: lÃª os arquivos baixados para o ambiente de processamento.
3. **TransformaÃ§Ã£o**: padroniza nomes de colunas, garantindo consistÃªncia entre os datasets.
4. **UnificaÃ§Ã£o**: concatena os dados, preenchendo eventuais informaÃ§Ãµes ausentes.
5. **ExportaÃ§Ã£o**: salva o resultado final na pasta `data_processed`.

## ğŸ–¥ï¸ Scripts e Classes

### `download_dados.py`
Realiza o download dos arquivos a partir de URLs, armazenando-os automaticamente na pasta `data_raw`.

### `processamento_dados.py`
ContÃ©m a classe `Dados` com mÃ©todos encadeados que executam toda a pipeline: carregar, transformar, unificar e salvar.

### `fusao_arquivos.py`
Script que instancia a classe `Dados` e executa a pipeline de forma direta, apÃ³s o download manual dos arquivos.

### `executar_pipeline.py`
Script completo que **baixa os arquivos** e **executa a pipeline** automaticamente de ponta a ponta, ideal para processos automatizados.

## ğŸ“Š Exemplo de ExecuÃ§Ã£o

Para rodar a pipeline completa, basta executar:

```bash
python scripts/executar_pipeline.py
```

Ao final, o arquivo dados_unificados.csv estarÃ¡ disponÃ­vel na pasta data_processed.

## Por que este projeto Ã© interessante?
**AutomaÃ§Ã£o**: do download ao processamento, tudo pode ser executado com um Ãºnico comando.
**Reusabilidade**: a classe Dados pode ser aplicada a outros projetos que exijam processos similares.
**OrganizaÃ§Ã£o**: os dados brutos e processados sÃ£o armazenados em pastas distintas, seguindo boas prÃ¡ticas de engenharia de dados.

## ExploraÃ§Ã£o dos Dados
Na pasta notebooks hÃ¡ um notebook exploratÃ³rio, onde foram realizadas anÃ¡lises iniciais para entender a estrutura dos dados e definir as transformaÃ§Ãµes necessÃ¡rias.

## Requisitos

- Python 3.10
- pandas
- requests
